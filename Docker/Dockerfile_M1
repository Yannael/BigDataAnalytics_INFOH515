# Container for Anaconda - Spark - Kafka  
# IMPORTANT: If you wish to share folder between your host and this container, make sure the UID for user guest is the same as your UID
# Check https://github.com/Yannael/brufence/blob/master/docker/streaming/README.md for details
# https://github.com/nodejs/help/issues/3239

FROM ubuntu:20.04 

# Needed for avoiding interactive prompt during tzdata installation (triggered by openssh-server - https://techoverflow.net/2019/05/18/how-to-fix-configuring-tzdata-interactive-input-when-building-docker-images/)
ENV DEBIAN_FRONTEND=noninteractive

# Install basic tools
RUN apt-get update
RUN apt-get install -y wget nano git unzip curl gcc openssh-server 

# Install Java. Java 8 preferable. Java 11 gives warnings about reflexive classes with PySpark
RUN apt-get install -y openjdk-8-jdk

# Create guest user. IMPORTANT: Change here UID 1000 to your host UID if you plan to share folders.
RUN useradd -ms /bin/bash guest -u 1000
RUN echo "guest:guest" | chpasswd

USER guest

ENV HOME /home/guest
WORKDIR $HOME

# Install Anaconda Python distribution
#RUN wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh
RUN wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-aarch64.sh
RUN bash Anaconda3-2021.11-Linux-aarch64.sh -b

# Install Spark (Spark 3.2.1 - 22/02/2022, prebuilt for Hadoop 3.2 or higher)
RUN wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
RUN tar xvzf spark-3.2.1-bin-hadoop3.2.tgz \
	&& mv spark-3.2.1-bin-hadoop3.2 spark \
	&& rm spark-3.2.1-bin-hadoop3.2.tgz

# Install Kafka
RUN wget https://dlcdn.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz
RUN tar xvzf kafka_2.13-3.0.0.tgz \
	&& mv kafka_2.13-3.0.0 kafka \
	&& rm kafka_2.13-3.0.0.tgz

# Install Kafka Python module
RUN $HOME/anaconda3/bin/pip install --upgrade pip
RUN $HOME/anaconda3/bin/pip install kafka-python 

# Issue with M1
#RUN pip install geopandas rtree

# Add environment variables
RUN echo "export SPARK_HOME=$HOME/spark" >> /home/guest/.bashrc
RUN echo "export KAFKA_HOME=$HOME/kafka" >> /home/guest/.bashrc
RUN echo "export PATH=$HOME/anaconda3/bin:$HOME/spark/bin:$HOME/spark/sbin:$HOME/kafka/bin:$PATH" >> /home/guest/.bashrc
RUN echo "export PYTHONPATH=$HOME/spark/python:$HOME/spark/python/lib/pyspark.zip:$HOME/spark/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH" >> /home/guest/.bashrc

# Add alias for notebook
RUN echo "alias notebook=\"jupyter notebook --ip='0.0.0.0' --NotebookApp.iopub_data_rate_limit=2147483647 --no-browser \" " >> /home/guest/.bashrc

USER root

# Startup (start Zookeeper, Kafka servers)
ADD kafka_startup_script.sh /usr/bin/kafka_startup_script.sh
RUN chmod a+x /usr/bin/kafka_startup_script.sh





