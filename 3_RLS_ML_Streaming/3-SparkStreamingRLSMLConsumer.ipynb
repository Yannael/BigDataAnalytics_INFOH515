{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [INFO-H515 - Big Data Scalable Analytics](https://uv.ulb.ac.be/course/view.php?id=85246?username=guest)\n",
    "\n",
    "## TP 3 - Streaming forecasting (RLS and ML) with Kafka and Spark Streaming\n",
    "\n",
    "#### *Gianluca Bontempi, Jacopo De Stefani and Theo Verhelst*\n",
    "\n",
    "####  29/04/2020\n",
    "\n",
    "\n",
    "## Streaming analytics with Kafka and Spark streaming \n",
    "\n",
    "This notebook provides an example for performing machine learning analytics on streaming data coming from a Kafka producer. \n",
    "\n",
    "The first part contains examples of simple __stateless__ processing scripts.\n",
    "\n",
    "The second part introduces __stateful__ streamining processing in order to perform more complex operations, like cumulative sum, sequential estimation of mean and variance.\n",
    "\n",
    "Predictive analytics is also implemented thanks to stateful processing.\n",
    "\n",
    "Two main __streaming learning__ strategies are proposed: \n",
    "* an online linear learning where he coefficient estimation is achieved using the __recursive least square (RLS)__ algorithm\n",
    "* a __mini-batch strategy__ where a learning model is fit to most recent data\n",
    "\n",
    "A __racing__ strategy is implemented as well where two different models (e.g. two linear models with different forgetting factors or a linear and a nonlinear regressor) are fitted to the same data.\n",
    "\n",
    "The data are produced in the notebook `KafkaTimeSeriesProducer`: two producers are considered, a linear one and a nonlinear one.\n",
    "\n",
    "This notebook uses:\n",
    "* the [Python client for the Apache Kafka distributed stream processing system](http://kafka-python.readthedocs.io/en/master/index.html) to receive messages from a Kafka cluster. \n",
    "* [Spark streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) for processing the streaming data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re, ast\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn import linear_model\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4050 '+\\\n",
    "                        '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 '+\\\n",
    "        '--conf spark.driver.memory=2g  pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"KafkaReceive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Function to connect Spark Streaming and Kafka on a given topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a connection to a Kafka stream\n",
    "#You may change the topic, or batch interval\n",
    "#The Zookeeper server is assumed to be running at 127.0.0.1:2181\n",
    "#The function returns the Spark context, Spark streaming context, and DStream object\n",
    "def getKafkaDStream(spark,topic='persistence',batch_interval=10):\n",
    "\n",
    "    #Get Spark context\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    #Create a DStream that represents streaming data from Kafka, for the required topic \n",
    "    dstream = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {topic: 1})\n",
    "    \n",
    "    return [sc,ssc,dstream]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure that Kafka is running and ready to receive messages:\n",
    "\n",
    "1. Launch `kafka_startup_script.sh` from a terminal.\n",
    "2. Check the kafka.log and zookeeper.log files in the `/home/guest`folder for the correct startup of the service.\n",
    "\n",
    "Let's have a look to the standard procedure while dealing with a Producer-Consumer architecture:\n",
    "\n",
    "1. Look at the data format in the producer (`KafkaTimeSeriesProducer`) notebook. \n",
    "2. Start the Data production by running the corresponding cell in the producer notebook. \n",
    "3. Move to the consumer notebook (i.e. this notebook).\n",
    "\n",
    "**N.B** Check that the names of the produced and receiver topic are the __same__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DStream object containing the streaming data sent on the dataLinearModel topic\n",
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=1)\n",
    "# Pretty print the content of dstream\n",
    "dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start data consumption you have to launch the Dstream. This is equivalent to opening a data tap on the receiving side.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "* Look at the time label: is it related to the batch interval you set? What about the data format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop data reception you have to stop the Dstream (close the tap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** Clear all outputs before starting the Streaming again with ssc.start()! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Change the batch interval value (in the consumer) and observe the impact on the buffer. What can you deduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateless processing\n",
    "\n",
    "The first examples in this notebook will introduce the concept of **stateless** processing.\n",
    "\n",
    "In this case, the Spark transformations are applied directly to the Discretized Stream (Dstream) produced by Kafka.\n",
    "\n",
    "A Discretized Stream (cf. [Spark Streaming Programming guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)) is simply a continuous collection of Spark RDD.\n",
    "\n",
    "Applying a transformation to a Dstream basically boils down to applying the same transformation to each underlying RDD.\n",
    "\n",
    "## Converting the data to a suitable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before being able to actually perform a transformation, we need to de-serialize the data obtained from Kafka.\n",
    "Since we want to do numerical processing, the raw data format is not suitable. \n",
    "Let's see an example of data conversion to a suitable numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=10)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Compare the suitable data format with the previous format. What is the main difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a map transformation to a Dstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples below apply a map transformation to each element of the batch without storing any state information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.pprint()\n",
    "maxS=dataS.map(lambda x: np.max(x[1:]))\n",
    "maxS.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Compare the content of the Dstream before and after the transformation. What is the applied transformation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Adapt the code of the previous exercise in order to compute the mean of the $x_i$ values at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom presentation of the received information \n",
    "\n",
    "Here you can find an example of using a `map` transformation to add some descriptive text to identify the received values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.map(lambda x: \"Stream counter : \"+ str(x[0])).pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first element of the transmitted data is the counter value, we can see which line is currently sent by the producer. If you do not stop the producer this value will continue growing. Given the stationary nature of our data set, this has not much of an impact for the following analysis...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the content of the Dstream to the disk you can use the Spark function `saveAsTextFiles(prefix, [suffix])`.\n",
    "\n",
    "The file name at each batch interval is generated based on the prefix and suffix parameters, as well as the current time in milliseconds: `prefix-<TIME_IN_MS>[.suffix]`.\n",
    "\n",
    "**Warning:** This command creates a new file for each batch: if the `batch_interval` value is low this can fill your directory rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=10)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.map(lambda x: \"stream counter=\"+ str(x[0])).pprint()\n",
    "prefix='test'\n",
    "dataS.map(lambda x: \"stream counter=\"+ str(x[0])).saveAsTextFiles(prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the streaming run for two epochs only and then look at your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example on how to visualize the directories containing the content of the processed batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dir = \"./\"\n",
    "files = list(filter(os.path.isdir, glob.glob(search_dir + prefix +\"*\")))\n",
    "files.sort(key=lambda x: os.path.getmtime(x))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each directory is splitted in several parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fyhat=files[0]+\"/part-00000\"\n",
    "F=open(fyhat, \"r\")\n",
    "F.read().replace(\"\\n\",\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fyhat=files[0]+\"/part-00001\"\n",
    "F=open(fyhat, \"r\")\n",
    "F.read().replace(\"\\n\",\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful processing\n",
    "\n",
    "As opposed to **stateless** processing, **stateful** process implies the existence of a state.\n",
    "\n",
    "According to the [Spark Streaming Programming guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html), the key concept for **stateful** processing is the `updateStateByKey`:\n",
    "\n",
    ">The `updateStateByKey` operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.\n",
    ">\n",
    ">    1. *Define the state* - The state can be an arbitrary data type.\n",
    ">    2. *Define the state update function* - Specify with a function how to update the state using the previous state and the new values from an input stream.\n",
    "\n",
    "The following examples will clarify how to create and initialize the state and how to perform updates of the state through the combination of `updateStateByKey` and user-defined functions `updateFunctions`.\n",
    "First example of stateful streaming processing obtained with an `updateFunction` called by the command __updateStateByKey__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single state value - No memory of the state\n",
    "\n",
    "In this first example, you will learn how to define the state as well as to code a simple `updateFunction` which takes into account only the first value in the streaming buffer and which does not perform a state update.\n",
    "\n",
    "Let's start by defining the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty vector with n elements\n",
    "n=11\n",
    "v=np.zeros((1,n)) \n",
    "\n",
    "state1=(v,0) # State is a 2 element tuple: (vector of size n,integer)\n",
    "\n",
    "# Transform the state into an RDD\n",
    "initialStateRDD = sc.parallelize([('state', state1)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by defining the user defined `updateFunction`.\n",
    "\n",
    "Note that all the update functions in the following cells share the same structure:\n",
    "\n",
    "```python \n",
    "def updateFunction(new_values, state): \n",
    "    L=len(new_values) # Size of the message buffer\n",
    "    if (L>0): # If there are new values in the buffer,\n",
    "        (...) = state # Unpacking the state\n",
    "        # Perform state update\n",
    "        # ...\n",
    "        # ...\n",
    "        return new_state  # Then return a new value for the state\n",
    "    else: # If there are no new values\n",
    "        return state # then return the unmodified state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state):\n",
    "    L=len(new_values) # size buffer\n",
    "    if (L>0):\n",
    "        (d, N) = state # Unpacking the state\n",
    "        value=new_values[0] ## take only the first value\n",
    "        d=np.abs(value[1:]).reshape(1,-1) ## d does NOT depend on the 'state' input\n",
    "        return (d,L)  ## return the value and size buffer\n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's conclude by applying the `updateFunction` on the Spark Streaming Dstream by means of the `updateStateByKey` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.pprint()\n",
    "\n",
    "dataS=dataS.flatMap(lambda x: [('state',1.0*np.array(x))])\n",
    "\n",
    "updatedS=dataS.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "updatedS.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Compare the raw data and the content of the state. What can you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single state value - With memory of the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the output of the function does not depend only on the current values in the buffer but also on the previous state. \n",
    "\n",
    "Let's start with defining the state and the `updateFunction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty vector with n elements\n",
    "n=11\n",
    "v=np.zeros((1,n)) \n",
    "\n",
    "state1=(v,0) # State is a 2 element tuple: (vector of size n,integer)\n",
    "\n",
    "# Transform the state into an RDD\n",
    "initialStateRDD = sc.parallelize([('state', state1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## Sum of absolute values of all vectors in the buffer\n",
    "    L=len(new_values)\n",
    "    if (L>0):\n",
    "        for l in np.arange(L):\n",
    "            value=new_values[l]\n",
    "            d=state[0]+np.abs(value[1:]).reshape(1,-1) ### d DOES depend  on state !!\n",
    "        return (d,L)  \n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Compare this `updateFunction` with the one in the section before. What can you observe?\n",
    "* What operation is being performed inside this `updateFunction`?\n",
    "* Run the code below to check your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.pprint()\n",
    "\n",
    "dataS=dataS.flatMap(lambda x: [('state',1.0*np.array(x))])\n",
    "\n",
    "updatedS=dataS.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "updatedS.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple values - with memory of the state\n",
    "\n",
    "Let's see here an example of usage of multiple values in the buffer (as opposed to the previous examples, only using the last value in the buffer).\n",
    "\n",
    "Note that the structure of the `updateFunction` remains the same as before, with the addition of a loop for iterating through the different elements of the buffer `new_values`.\n",
    "\n",
    "```python \n",
    "def updateFunction(new_values, state): \n",
    "    L=len(new_values) # Size of the message buffer\n",
    "    if (L>0): # If there are new values in the buffer,\n",
    "        (...) = state # Unpacking the state\n",
    "        for l in np.arange(L):\n",
    "            value=new_values[l]\n",
    "            # Perform state update\n",
    "            # ...\n",
    "            # ...\n",
    "        return new_state  # Then return the updated state\n",
    "    else: # If there are no new values\n",
    "        return state # then return the unmodified state\n",
    "```\n",
    "\n",
    "In this case, all the different values of the state are stacked in a matrix.\n",
    "Beware that, with this implementation, the size of this matrix keeps growing indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty vector with n elements\n",
    "n=11\n",
    "v=np.zeros((1,n)) \n",
    "\n",
    "state1=(v,0) # State is a 2 element tuple: (vector of size n,integer)\n",
    "\n",
    "# Transform the state into an RDD\n",
    "initialStateRDD = sc.parallelize([('state', state1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state):\n",
    "    L=len(new_values)  ## size of the buffer\n",
    "    if (L>0):\n",
    "        D=state[0]\n",
    "        for l in np.arange(L): ## loop over all the values of the buffer\n",
    "            value=new_values[l]\n",
    "            D=np.vstack((D,value[1:].reshape(1,-1)))\n",
    "        return (D,D.shape,L)  \n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.pprint()\n",
    "\n",
    "dataS=dataS.flatMap(lambda x: [('state',1.0*np.array(x))])\n",
    "\n",
    "updatedS=dataS.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "updatedS.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential estimation of sampled means and variances by state update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex example of stateful processing is given by the sequantial estimation of the mean of a random variable.\n",
    "\n",
    "Average batch formulation: $\\mu_{(N)}=\\frac{1}{N} \\sum_{i=1}^N z_i $\n",
    "\n",
    "Average sequential formulation:  $\\mu_{(N)}=\\mu_{(N-1)} +\\frac{1}{N} (z_N -\\mu_{(N-1)} ) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muhat=0\n",
    "N=0\n",
    "state1=(muhat,N)\n",
    "## state with two components: estimate of mu and number of samples\n",
    "\n",
    "initialStateRDD = sc.parallelize([('state', state1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## Update the sequential estimate of sample mean and sample variance of x[0] \n",
    "    ## (i.e. second element of value vector, the first being the state)\n",
    "    L=len(new_values) ## size of the buffer\n",
    "    if (L>0 ):\n",
    "        (muhat,N) = state # Unpacking the state\n",
    "        for l in np.arange(L):\n",
    "            N=N+1\n",
    "            value=new_values[l]\n",
    "            muhatold=muhat\n",
    "            muhat=muhat+1.0/N*(value[2]-muhat)\n",
    "        return (muhat,N)          \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.map(lambda x: \"stream counter=\"+ str(x[0])).pprint()\n",
    "\n",
    "dataS=dataS.flatMap(lambda x: [('state',1.0*np.array(x))])\n",
    "dataS.map(lambda x: \"Input=\"+ str(x)).pprint()\n",
    "\n",
    "updatedS=dataS.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "updatedS.map(lambda x : 'After '+str(x[1][1])+' samples: muhat='+str(x[1][0])).pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.**: With the dataLinearModel topic, mean estimation must converge to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Modify the state and the update function to perform the sequential computation of the variance. \n",
    "\n",
    "The state should contain two additional variables:  $\\sigma^2_{(N)}$ and $S_{(N)}$.\n",
    "\n",
    "The formulas definining the state update are the following ones:\n",
    "\n",
    "- Sample variance batch formulation: $\\sigma^2_{(N)}=\\frac{1}{N} \\sum_{i=1}^N (z_i -\\mu_{(N)})^2= \n",
    "\\left(\\frac{1}{N} \\sum_{i=1}^N z_i^2 \\right) -  \\left(\\frac{1}{N} \\sum_{i=1}^N z_i \\right) ^2 $\n",
    "\n",
    "- Sample variance sequential formulation:  $\\sigma^2_{(N)} ={S_{(N)}/N}$ where $$S_{(N)}= S_{(N-1)}+N(N-1) (\\mu_{(N)}- \\mu_{(N-1)})^2 $$ \n",
    "\n",
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution - State\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## Your solution\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.**: With the dataLinearModel topic, mean estimation must converge to 0. Variance estimation must converge to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Least Squares (streaming - single model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following equations define a single step of the Recursive Least Squares (with forgetting factor $\\nu$).\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "V_{(t)}&=\\frac{1}{\\nu} \\left(V_{(t-1)}\n",
    "-\\frac{V_{(t-1)} x^T_{t} x_{t} V_{(t-1)}}{1+ x_{t} V_{(t-1)} x^T_{t}} \\right)\\\\[3pt]\n",
    "\\alpha_{(t)}&= V_{(t)} x^T_{t} \\\\[3pt]\n",
    "e&= y_{t}- x_{t} \\hat{\\beta}_{(t-1)}  \\\\[3pt]\n",
    "\\hat{\\beta}_{(t)}&=\\hat{\\beta}_{(t-1)}+ \\alpha_{(t)} e \\\\[3pt]\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "where $V$ is the covariance matrix and $\\beta$ is the set of parameters of the linear model.\n",
    "Details in the Streaming Analytics slides.\n",
    "\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "The implementation of the state and an `updateFunction` to implement RLS in a streaming fashion is given below.\n",
    "\n",
    "* Write the implementation of `RLSstep` function in order to perform the update of the state of the RLS model according to the equations given above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLSstep(y,x,beta,V,nu):\n",
    "    ## Your solution\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10 # number of features\n",
    "beta1=np.zeros(n+1)  ## initial parameter vector for model 1\n",
    "v0=10 ## initialization covariance\n",
    "V1=np.diag(np.zeros(n+1)+v0) ## initial covariance matrix for model 1\n",
    "nu1=1.0 # forgetting factor for model 1\n",
    "\n",
    "recentSize=10\n",
    "D=np.zeros((recentSize,n+1)) #np.random.rand(recentSize,n+1) #np.zeros((20,n+1))+1\n",
    "E=np.zeros((1,1))\n",
    "mse=0\n",
    "N=0\n",
    "state1=(beta1,V1,nu1,mse,N,D,E)\n",
    "initialStateRDD = sc.parallelize([('rls', state1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## RLS update function\n",
    "    ## Only update with first value of RDD.\n",
    "    L=len(new_values)\n",
    "    \n",
    "    if (L>0):\n",
    "        # Extract the data from state variable\n",
    "        (beta,V,nu,sse,N,DD,E) = state\n",
    "        #beta=state[0] ## Beta value\n",
    "        #V=state[1]   ## V matrix\n",
    "        #nu=state[2]   ## Forgetting factor\n",
    "        #sse=state[3]  ## Sum of squared errors\n",
    "        #N=state[4]   ## Number of treated samples\n",
    "        #DD=state[5]  ## Analyzed values\n",
    "        #E=state[6]   ## Error measure\n",
    "        \n",
    "        # Extract the values from the new_values variable\n",
    "        yx=new_values[0] #Only using the first element in the batch\n",
    "        i=yx[0] \n",
    "        y=yx[1]\n",
    "        x=yx[2:]\n",
    "        n=len(x)\n",
    "        beta.shape=(n+1,1)\n",
    "        \n",
    "        # Compute RLS state update\n",
    "        RL=RLSstep(y,x,beta,V,nu)\n",
    "        \n",
    "        # Update the state values using results from RLSstemp\n",
    "        # Update beta and V values\n",
    "        beta=RL[0]\n",
    "        V=RL[1]\n",
    "        E=np.append(E,RL[2])\n",
    "        \n",
    "        # Update sum of squares - for MSE computation\n",
    "        sse=sse+pow(RL[2],2.0)\n",
    "        \n",
    "        # Append analyzed values to DD matrix\n",
    "        d=yx[1:]\n",
    "        d.shape=(1,n+1)\n",
    "        DD=np.vstack((d,DD[:-1,:]))\n",
    "        \n",
    "        return (beta,V,nu,sse/(N+1.0),N+1,DD,E)  ## update formula mod1\n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "\n",
    "dataS=dataS.flatMap(lambda x: [('rls',1.0*np.array(x))])\n",
    "\n",
    "updatedS=dataS.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "\n",
    "## printing out updated values of the state\n",
    "outbetaS=updatedS.map(lambda x: \": beta=\"+ np.array2string(x[1][0])).pprint()\n",
    "outmseS=updatedS.map(lambda x: 'mse=' + str(x[1][3])).pprint()\n",
    "outNS=updatedS.map(lambda x: x[1][4]).pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "If you look at the code above you will see that there is no guarantee that all samples are used. \n",
    "For instance, if the length `L` of the buffer in `new_values` is bigger than one (due to an excessive latency of the reception), the algorithm above will consider only the first vector `yx`. \n",
    "\n",
    "* How should the code be modified in order to consider all the elements of the buffer 'new_values'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLSstep(y,x,beta,V,nu):\n",
    "    ## Your solution\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution - State\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## Your solution\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Least Squares (streaming - multiple models in parallel)\n",
    "\n",
    "The code in the previous section can be extended to support multiple models running in parallel.\n",
    "\n",
    "* The *state* is modified in order to include the states of the two different models.\n",
    "    Each state contains a state of variables to keep the state of the model, as well as to keep track of MSE estimates. \n",
    "    A state is a list of 5 elements:\n",
    "    * The first three are beta, V and mu, and define the state of the model (see RLS formulas in course)\n",
    "    * The last two are an estimate of the MSE of the model, and the number of treated samples\n",
    "   The two states are combined in a key-value RDD in the following way:\n",
    "   ```python\n",
    "    initialStateRDD = sc.parallelize([(u'rls1', state1),(u'rls2', state2)])\n",
    "   ```\n",
    "   with `rlsX` being the key and the value being the corresponding state\n",
    "\n",
    "* The original Dstream is duplicated using a `flatMap` operation in order to create two key-pair streams, containing the key corresponding to the model, as well as a copy of the data in the original Dstream (`dataS` here).\n",
    "```python\n",
    "dataS2=dataS.flatMap(lambda x: [('rls1',('rls1',1.0*np.array(x))),\n",
    "                            ('rls2',('rls2',1.0*np.array(x)))])\n",
    "```\n",
    "\n",
    "* The *update function* is modified to handle the new format of the stream (by means of the `key` variable), by storing this value from the incoming state and re-transmitting it in the updated state.\n",
    "\n",
    "* Note that there is only a single `updateFunction`, since the `updateStateByKey` operation applies the update function in parallel to all the different keys (in this case, each key correspond to a model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLSstep(y,x,n,beta,V,nu):\n",
    "    x.shape=(1,n)\n",
    "    x1=np.append(1,x)\n",
    "    x1.shape=(1,n+1)\n",
    "    \n",
    "    V=1.0/nu*(V-V.dot(x1.T).dot(x1).dot(V)/(1.0+float(x1.dot(V).dot(x1.T))))\n",
    "    alpha=V.dot(x1.T)\n",
    "    yhat=x1.dot(beta)\n",
    "    err=y-yhat\n",
    "    beta=beta+alpha*err\n",
    "    \n",
    "    return(beta,V,err,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10 # number of features\n",
    "beta1=np.zeros(n+1)  ## initial parameter vector for model 1\n",
    "v0=10 ## initialization covariance\n",
    "V1=np.diag(np.zeros(n+1)+v0) ## initial covariance matrix for model 1\n",
    "nu1=1.0 # forgetting factor for model 1\n",
    "\n",
    "batchSize=10\n",
    "D=np.zeros((batchSize,n+1)) ## this is not used here \n",
    "E=np.zeros((1,1))\n",
    "mse=0\n",
    "N=0\n",
    "state1=('mod1',beta1,V1,nu1,mse,N,D,E,0,0)\n",
    "\n",
    "nu2=0.99 # forgetting factor for model 2\n",
    "\n",
    "## Note that the only difference is the forgetting value\n",
    "state2=('mod2',beta1,V1,nu2,mse,N,D,E,0,0)\n",
    "\n",
    "initialStateRDD = sc.parallelize([(u'rls1', state1),\n",
    "                                  (u'rls2', state2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## RLS update function\n",
    "    ## Only update with first value of RDD. \n",
    "    L=len(new_values)  ## size of the buffer\n",
    "    if (L>0):\n",
    "        # Extract the data from state variable\n",
    "        (model_key,beta,V,nu,sse,N,DD,E,y,yhat) = state\n",
    "        #model_key=state[0] ## Unique identifier for the model\n",
    "        #beta=state[1] ## Beta value\n",
    "        #V=state[2]   ## V matrix\n",
    "        #nu=state[3]   ## Forgetting factor\n",
    "        #sse=state[4]  ## Sum of squared errors\n",
    "        #N=state[5]   ## Number of treated samples\n",
    "        #DD=state[6]  ## Analyzed values\n",
    "        #E=state[7]   ## Error measure\n",
    "        #y=state[8]   ## True value for the last value in the batch\n",
    "        #yhat=state[9] ## Predicted value for the last value in the batch\n",
    "        \n",
    "        # Extract data from the new_values vector\n",
    "        # new_values = (key,RDD([counter,y,x]))\n",
    "        key=new_values[0][0]\n",
    "        yx=new_values[0][1]   ### this is only the first value in the buffer\n",
    "        i=yx[0]\n",
    "        y=yx[1]\n",
    "        x=yx[2:]\n",
    "        n=len(x)\n",
    "        beta.shape=(n+1,1)\n",
    "        \n",
    "        # Compute RLS state update\n",
    "        RL=RLSstep(y,x,n,beta,V,nu)\n",
    "        \n",
    "        # Update the state values using results from RLSstep\n",
    "        beta=RL[0]\n",
    "        V=RL[1]\n",
    "        err=RL[2]\n",
    "        E=np.append(E,err)\n",
    "        yhat=RL[3]\n",
    "        \n",
    "        # Update sum of squares - for MSE computation\n",
    "        sse=sse+pow(err,2.0)\n",
    "         \n",
    "        # Append analyzed values to DD matrix\n",
    "        d=yx[1:]\n",
    "        d.shape=(1,n+1)\n",
    "        DD=np.vstack((d,DD[:-1,:]))\n",
    "        \n",
    "        return (key,beta,V,nu,sse/(N+1.0),N+1,DD,E,y,yhat)  \n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "\n",
    "dataS.map(lambda x: \"stream counter=\"+ str(x[0])).pprint()\n",
    "\n",
    "dataS2=dataS.flatMap(lambda x: [('rls1',('rls1',1.0*np.array(x))),\n",
    "                            ('rls2',('rls2',1.0*np.array(x)))])\n",
    "\n",
    "updatedS=dataS2.updateStateByKey(updateFunction,initialRDD=initialStateRDD)#.pprint()\n",
    "\n",
    "## printing out updated values of the states of the two models\n",
    "outbetaS=updatedS.map(lambda x: x[1][0]+\": beta=\"+ np.array2string(x[1][1])).pprint()\n",
    "outmseS=updatedS.map(lambda x: x[1][0]+\": mse=\"+ np.array2string(x[1][4])).pprint()\n",
    "outNS=updatedS.map(lambda x: x[1][0]+\": N=\"+ str(x[1][5])).pprint()\n",
    "outyS=updatedS.map(lambda x: x[1][0]+\": y=\"+ str(x[1][8])+ \" yhat=\"+ str(x[1][9][0])).pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "If you look at the code above you will see that there is no guarantee that all samples are used. \n",
    "For instance, if the length `L` of the buffer in `new_values` is bigger than one (due to an excessive latency of the reception), the algorithm above will consider only the first vector `yx`. \n",
    "\n",
    "* How should the code be modified in order to consider all the element of the buffer 'new_values'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLSstep(y,x,beta,V,nu):\n",
    "    ## Your solution\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution - State\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## Your solution\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML batch predictor (racing of several models) \n",
    "\n",
    "To conclude this session, below you can find an example of the implementation of the parallel racing of multiple machine learning predictors (here *Random Forests* - `rf`, *Linear Model* - `lin`, *Gradient boosting* - `gb`, respectively).\n",
    "\n",
    "As you can observe, the general structure of the state and the `updateFunction` is the same as the previous section, with:\n",
    "\n",
    "* The *state* is modified in order to include the states of the three models.\n",
    "    A state is a list of 4 elements:\n",
    "    * `D` - The data batch to predict\n",
    "    * `mse` - Current value for the MSE\n",
    "    * `N` - Counter of the values \n",
    "    * `E` - Vector of historical error measues\n",
    "   The state of each model is then combined in a key-value RDD with a key identifying the type of model (cf. `state1,state2,state3`).\n",
    "   \n",
    "* The original Dstream is duplicated using a `flatMap` operation in order to create one key-pair streams for each considered model \n",
    "\n",
    "* The *update function* is modified to employ the predictors from Scikit-learn to predict the last value for the batch (cf. `predict` function definition), in a leave-one-out fashion, with the training of the predictor being performed on the batch of previously collected data (`DD` variable).\n",
    "\n",
    "* Note that there is only a single `updateFunction`, since the `updateStateByKey` operation applies the update function in parallel to all the different keys (in this case, each key correspond to a model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10 # number of features\n",
    "\n",
    "batchSize=50\n",
    "D=np.zeros((batchSize,n+1)) \n",
    "## batch data size\n",
    "E=np.zeros((1,1))\n",
    "mse=0\n",
    "N=0\n",
    "state1=('rf',D,mse,N,E,0,0)\n",
    "state2=('lin',D,mse,N,E,0,0)\n",
    "state3=('gb',D,mse,N,E,0,0)\n",
    "\n",
    "\n",
    "initialStateRDD = sc.parallelize([(u'rf', state1),\n",
    "                                  (u'lin', state2),\n",
    "                                 (u'gb', state3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mod,YX,q):\n",
    "    N=YX.shape[0]\n",
    "    Ntr=int(np.round(N/2))\n",
    "    Ytr=YX[:Ntr,0]\n",
    "    Xtr=YX[:Ntr,1:]\n",
    "    Yts=YX[Ntr:,0]\n",
    "    Xts=YX[Ntr:,1:]\n",
    "    Nts=Xts.shape[0]\n",
    "    if mod==\"rf\":\n",
    "        regr = RandomForestRegressor(random_state=0)\n",
    "    if mod==\"gb\":    \n",
    "        regr = GradientBoostingRegressor(random_state=0)\n",
    "    if mod==\"lin\":\n",
    "        regr=LinearRegression()\n",
    "    regr.fit(Xtr, Ytr)\n",
    "    Yhat=regr.predict(q)\n",
    "   \n",
    "\n",
    "    return(Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    L=len(new_values)\n",
    "    \n",
    "    if (L>0):\n",
    "        # Extract the data from state variable\n",
    "        (model_key,DD,mse,N,E,y,yhat) = state\n",
    "        #model_key=state[0] ## Unique identifier for the model\n",
    "        #DD=state[1]  ## Analyzed values\n",
    "        #mse=state[2]  ## Mean squared errors\n",
    "        #N=state[3]   ## Number of treated samples\n",
    "        #E=state[4]   ## Error measure\n",
    "        #y=state[5]   ## True value for the last value in the batch\n",
    "        #yhat=state[6] ## Predicted value for the last value in the batch\n",
    "\n",
    "        key=new_values[0][0]\n",
    "          \n",
    "        ## prediction of the output of the most recent value\n",
    "        yx=new_values[-1][1]      \n",
    "        i=yx[0]\n",
    "        y=yx[1]\n",
    "        x=yx[2:]\n",
    "        x.shape=(1,n)\n",
    "        N=N+1\n",
    "        \n",
    "        yhat=predict(key,DD,x) \n",
    "        err=y-yhat  ## prediction error for the latest values\n",
    "        mse=mse+1.0/N*(pow(err,2.0)-mse) ## sequential update of MSE\n",
    "        E=np.append(E,err)        \n",
    "        \n",
    "        ## batch update\n",
    "        for l in np.arange(L):\n",
    "            yx=new_values[l][1]      \n",
    "            d=yx[1:]\n",
    "            d.shape=(1,n+1)\n",
    "            DD=np.vstack((d,DD[:-1,:]))\n",
    "        return (key,DD,mse,N,E,y,yhat)  \n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataNonLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "\n",
    "dataS.map(lambda x: \"stream counter=\"+ str(x[0])).pprint()\n",
    "\n",
    "dataS2=dataS.flatMap(lambda x: [('rf',('rf',1.0*np.array(x))),\n",
    "                            ('lin',('lin',1.0*np.array(x))),\n",
    "                            ('gb',('gb',1.0*np.array(x)))\n",
    "                               ])\n",
    "updatedS=dataS2.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "\n",
    "## printing out updated values of the states of the two models\n",
    "outmseS=updatedS.map(lambda x: x[1][0]+\": mse=\"+ str(x[1][2])).pprint()\n",
    "outNS=updatedS.map(lambda x: x[1][0]+\": N=\"+ str(x[1][3])).pprint()\n",
    "outyS=updatedS.map(lambda x: x[1][0]+\": y=\"+ str(x[1][5])+ \" yhat=\"+ str(x[1][6])).pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the batch is empty at the beginning. So the prediction becomes more accurate (and the MSE smaller) only when  the batch dataset if filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last but not least: debugging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty in programming streaming analytics code is the debugging of the code. Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WRONGupdateFunction(new_values, state): \n",
    "    L=len(new_values)\n",
    "    ## size buffer\n",
    "    if (L>0):\n",
    "        ## value=new_values[0] ## CORRECT: take the first value\n",
    "        value=new_values   ## WRONG\n",
    "        d=np.abs(value[1:]).reshape(1,-1)+state[0] \n",
    "        return (d,L)  \n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=2)\n",
    "dataS = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "dataS.map(lambda x: \"stream counter=\"+ str(x[0])).pprint()\n",
    "\n",
    "dataS=dataS.flatMap(lambda x: [('state',1.0*np.array(x))])\n",
    "\n",
    "n=11\n",
    "v=np.zeros((1,n))\n",
    "\n",
    "state1=(v,0)\n",
    "## state with two components: vector of size 11 and an integer\n",
    "\n",
    "initialStateRDD = sc.parallelize([('state', state1)])\n",
    "\n",
    "updatedS=dataS.updateStateByKey(WRONGupdateFunction,initialRDD=initialStateRDD)\n",
    "updatedS.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the wrong code you will see on the **notebook outbook** only the output of `pprint()` command but not the second output. \n",
    "\n",
    "At the same time if you go to the **terminal window** you will see a long series of messages like:\n",
    "\n",
    "```\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
    "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
    "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
    "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
    "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
    "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
    "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
    "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
    "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
    "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\n",
    "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
    "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)\n",
    "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)\n",
    "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
    "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of those messages are very _low level_ and of no help but some refer to your actual code like\n",
    "   \n",
    "  **File \"<ipython-input-141-72bbbd66e1c1>\", line 7, in WRONGupdateFunction**\n",
    "    \n",
    "**ValueError: operands could not be broadcast together with shapes (1,372) (1,11)**\n",
    "\n",
    "```\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem was indeed in line 7 of your update function...\n",
    "Correct it and launch again.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
