{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [INFO-H515 - Big Data Scalable Analytics](https://uv.ulb.ac.be/course/view.php?id=85246?username=guest)\n",
    "\n",
    "## TP 2 - Linear Regression with Map Reduce\n",
    "\n",
    "*Materials orignally developed by Yann-Aël Le Borgne, Jacopo De Stefani and Gianluca Bontempi*\n",
    "\n",
    "#### *Jacopo De Stefani, Theo Verhelst and Gianluca Bontempi*\n",
    "\n",
    "####  22/04/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class aims at implementing from scratch map-reduce solutions to linear regression problems. Linear regression consists in approximating an output (response) variable $y$ by a linear combination of a set of inputs (dependent variables) $x_i$.\n",
    "\n",
    "![](./img/ExampleLinReg.png)\n",
    "\n",
    "The two main approaches to solve linear regression problems are the Ordinary Least Squares (OLS), and the Gradient Descent (GD). \n",
    "\n",
    "### Class objectives:\n",
    "\n",
    "* Review the solving of a linear regression problem using the standard ML sklearn Python toolbox\n",
    "* Review the ordinary least square (OLS) solution. Implement it from scratch using numpy\n",
    "* Implement the Map/Reduce OLS solution using Spark\n",
    "* Review the Gradient Descent (GD) solution. Implement it from scratch using numpy\n",
    "* Implement the Map/Reduce GD solution using Spark\n",
    "* Solve the problem using Spark ML library - LinearRegression, and its OLS and gradient descent implementations\n",
    "* Interpret Spark optimisation plan and algorithm distribution in the Spark user interface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression - Background theory\n",
    "\n",
    "* Linear regression is a widely used method in predictive analytics.\n",
    "* It consists in approximating an output by a weighted sum of the inputs, in such a way that the mean square error between the predicted and true outputs is minimized.\n",
    "\n",
    "### Notations\n",
    "* Let $(x_i,y_i)$, $1 \\le i \\le N$, be a set of $N$ observations, with $x_i \\in \\mathbb{R}^n$ and $y_i \\in \\mathbb{R}$. $x_i$ are the inputs, and $y_i$ the outputs.\n",
    "* Let $X$ be the matrix of inputs with rows $x_i$, of size $N \\times n$ ($N$ observations, (rows) and $n$ features (columns)). \n",
    "* Let $Y \\in \\mathbb{R}^N$ be the vector of outputs $y_i$.\n",
    "* Let $\\theta \\in \\mathbb{R}^n$ be the parameters of a linear model $\\hat{y_i}=\\theta x_i^T$.\n",
    "* Let $L(y_i, \\hat{y}_i)=\\frac{1}{2}(y_i-\\hat{y}_i)^2$ be the mean squared loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solutions to the regression problems\n",
    "\n",
    "#### Ordinary least squares\n",
    "\n",
    "[It can be shown](https://theclevermachine.wordpress.com/2012/09/01/derivation-of-ols-normal-equations/) that the set of parameters $\\hat{\\theta}$ which minimize the loss function $L$ over the whole set of observations $(X,Y)$ is given by\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^T = (X^T X)^{-1} (X^T Y)\n",
    "$$\n",
    "\n",
    "This is the **Ordinary Least Square** solution, which is the closed form solution.\n",
    "\n",
    "#### Gradient descent \n",
    "\n",
    "The closed form solution (OLS) requires a matrix inversion, which is not possible if $X^T X$ is singular (which happens if $n>N$ or if a feature is a linear combination of others). An alternative to the OLS method is the  Gradient Descent (GD) method. The linear regression weights are first drawn at random. They are then iteratively updated by summing and averaging the gradients for all $x_i$ in $X$.    \n",
    "\n",
    "``` \n",
    "GD(X,Y,T,η)\n",
    "N<-nrow(X)\n",
    "theta_0 <- random initialization\n",
    "for t = 1 to T do\n",
    "    sum_gradient=0\n",
    "    for i = 1 to N\n",
    "        sum_gradient=sum_gradient − ηL'(x_i,y_i)\n",
    "    end_for\n",
    "    theta_t ← theta_(t−1)+sum_gradient/N\n",
    "end_for\n",
    "return theta_t\n",
    "``` \n",
    "$L'$ is the derivative of $L$. For the squared loss, we have $L'(y_i, \\hat{y}_i)=(x_i \\theta^T-y_i)x_i$.\n",
    "\n",
    "Note that the GD approach requires two hyperparameters: The number of iteration $T$, and the learning rate $η$, which controls how 'fast' the linear weights are updated. Tuning $T$ and $η$ to efficiently solve the problem is in practice often difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map/Reduce\n",
    "\n",
    "If $X$ and $Y$ do not fit in memory, Map/Reduce can be used to solve the linear regression problem in a distributed way. Two cases can be distinguished:\n",
    "\n",
    "* ** $N$ large and $n$ 'small' ** : $X^TX$ and $X^TY$ are of size $n \\times n$ and $n \\times 1$, respectively, and can be assumed to be 'small' enough to fit in the memory of a single machine. Only the matrix products $X^TX$ and $X^TY$ are computed with Map/Reduce in a distributed way. The matrix inversion $(X^TX)^{-1}$ and resulting product $\\hat{\\theta}^T = (X^T X)^{-1} (X^T Y)$ are then computed on a single machine.\n",
    "\n",
    "* ** $n$ 'large' ** : The matrix product $X^TX$ has size $n^2$ which can become too large to fit on a single machine. The risk that $X^TX$ is singular and that the inverse cannot be computed is also increased. The Map/Reduce GD method must be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using mostly numpy and Spark RDD libraries. For reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant [NumPy](https://docs.scipy.org/doc/numpy/reference/index.html) methods in the NumPy Reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Dataset generation\n",
    "\n",
    "Let us first generate an artificial dataset of $N$ observations where the output $y$ is a linear combination of $n$ inputs plus gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(N,n,random_seed):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    np.random.seed(0)   \n",
    "\n",
    "    #Inputs and the weights of the linear combination are drawn at random\n",
    "    X=np.random.rand(N,n)\n",
    "    theta=np.random.rand(n)\n",
    "    #noise=np.random.rand(N)\n",
    "\n",
    "    Y=np.dot(X,theta)#+noise\n",
    "    Y=Y[:,np.newaxis]\n",
    "    Z=np.concatenate((X,Y),axis=1)\n",
    "\n",
    "    print(\"Number of observations :\",N)\n",
    "    print(\"Number of features :\",n)\n",
    "\n",
    "    print(\"Dimension of X :\",X.shape)\n",
    "    print(\"Dimension of theta :\",theta.shape)\n",
    "    print(\"Dimension of Y :\",Y.shape)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time to create artificial data: \",round(end - start,2),\"seconds\")\n",
    "    \n",
    "    return (X,Y,Z,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let us generate the dataset\n",
    "N=100\n",
    "n=2\n",
    "(X,Y,Z,theta)=genData(N,n,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning: Do not plot if N>1000 !\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[0:N,0],X[0:N,1],Y[0:N])\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check theta values\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ordinary Least Squares\n",
    "\n",
    "Let us first solve the linear regression using standard approaches:\n",
    "\n",
    "* Using Python sklearn and the sklearn.linear_model function\n",
    "* Using the least mean square equation with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Centralised approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Parameter identification using sklearn\n",
    "\n",
    "Let us compute the model coefficients with the [sklearn.linear_model](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(X,Y)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Regression coefficient found with sklearn:\",model.coef_)\n",
    "print(\"Time to fit linear model with sklearn: \",round(end - start,4),\"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Parameter identification using closed form and numpy\n",
    "\n",
    "Let us compute the model coefficients using the OLS solution, with numpy [`dot`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html), [`transpose`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html), and [`linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) functions.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Compute $X^TX$ using the numpy `dot` function, and store result in a variable `XtX`\n",
    "* Compute $X^TY$ using the numpy `dot` function, and store result in a variable `XtY`\n",
    "* Compute the inverse of $X^TX$ using the numpy `linalg.inv` function, and store result in a variable `XtX_inverse`\n",
    "* Compute coefficient $\\hat{\\theta}=(X^T X)^{-1} (X^T Y)$ using the numpy `dot` function and store the result in a variable theta_hat\n",
    "* Check that theta_hat is the same as the result found with `sklearn.linear_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Map/Reduce approach for OLS\n",
    "\n",
    "Summary of the approach:\n",
    "\n",
    "* $X^T X$ and $X^T Y$ are computed in a distributed way.\n",
    "* $X^T X$ and $X^T Y$ have size $n \\times n$ and $n$, with $n$ small. $(X^T X)^{-1} (X^T Y)$ can be computed locally (no need to distribute the computation of the matrix inverse).\n",
    "\n",
    "#### Distribution of the computation of $X^T X$ and $X^T Y$\n",
    "\n",
    "* Let $Z=[X|Y]$ be the $X$ matrix augmented with $Y$ as the last column.\n",
    "* Let $Z$ be partitioned into $B$ row blocks $Z_k$ of size $N_B \\times (n+1)$\n",
    "\n",
    "$$Z = \\left[\\begin{array}\n",
    "{r}\n",
    "Z_1  \\\\\n",
    "Z_2  \\\\\n",
    "...\\\\\n",
    "Z_B\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distributed implementation will consist in computing $X^TX$ and $X^TY$ in parallel using the block $Z_k$**.\n",
    "\n",
    "* **Stage 1**: Partitioning. Partition $Z$ in $B$ blocks $Z_k$, $1 \\le k \\le B$. This can be done when calling the `parallelize` function to create the Spark Resilient Distributed Dataset (RDDs). \n",
    "* **Stage 2**: Computation of partial $X^TX$ with mapping. For each row of the partition $Z_k$, return `xtx`$= x_k^T x_k$ (size $n \\times n$).\n",
    "* **Stage 3**: Reduce. Sum all `xtx`.\n",
    "* **Stage 4**: Computation of partial $X^TY$ with mapping. For each row of the partition $Z_k$, return `xty`$= x_k^T y_k$ (size $n \\times 1$).\n",
    "* **Stage 5**: Reduce. Sum all `xty`.\n",
    "* **Stage 6**: Collect. Collect the reduced XtX and XtY, and compute $\\hat{\\theta}^T = (X^T X)^{-1} (X^T Y)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Start Spark session with local master and 2 cores\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"LinearRegression\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD with B partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us use 2 partitions\n",
    "B=2\n",
    "Z_RDD=sc.parallelize(Z,B).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_RDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "* Open the Spark UI (yourIP:4040), check the DAG visualisation and event timeline\n",
    "* Change the number to B=5 partitions et reparallelize Z. What change do you expect in the event timeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Approach 1: map function\n",
    "\n",
    "The standard approach for Map/Reduce consists in applying a map function to each row of an RDD using the `map` operator, and reduce the results using the `reduce` operator. Below is an example of the Map/Reduce implementation for computing `XtX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xtx_row(z):\n",
    "    #x is all columns except the last one (y)\n",
    "    x=z[:-1]\n",
    "    xtx=np.outer(x,x)\n",
    "    return xtx\n",
    "\n",
    "XtX=Z_RDD.map(xtx_row).reduce(lambda x,y:(x+y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "    \n",
    "* What is the shape (number of rows/columns) of the resulting RDD?\n",
    "* Check the DAG, event timeline, and number of records/partitions in the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Apply the same approach as `xtx_row` for computing `XtY`. Call that function `xty_row`\n",
    "* Compute the coefficients $\\hat{\\theta}=(X^T X)^{-1} (X^T Y)$ using the numpy `dot` and `linalg.inv` functions and store the result in a variable theta_hat\n",
    "* Check that theta_hat is the same as the result found with `sklearn.linear_model` (no map/reduce needed here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Approach 2 : map function with combined computation of `xtx` and `xty`\n",
    "\n",
    "Instead of using to separate map/reduce for computing `XtX` and `XtY`, both can be computed in a single map/reduce (since x and y are in z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Modify the `xtx_row` function in a `xtx_xty_row` function so that `xtx` and `xty` are computed in a single map. The output should be a tuple `(xtx,xty)`\n",
    "* Modify the map/reduce pipeline so the `xtx_xty_row` is applied, and adapt the reduce part so both elements `(xtx,xty)` are summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xtx_xty_row(z):\n",
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3) Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Centralised approach\n",
    "\n",
    "``` \n",
    "GD(X,Y,T,η)\n",
    "N<-nrow(X)\n",
    "theta_0 <- random initialization\n",
    "for t = 1 to T do\n",
    "    sum_gradient=0\n",
    "    for i = 1 to N\n",
    "        sum_gradient=sum_gradient − ηL'(x_i,y_i)\n",
    "    end_for\n",
    "    theta_t ← theta_(t−1)+sum_gradient/N \n",
    "end_for\n",
    "return theta_t \n",
    "``` \n",
    "$L'$ is the derivative of $L$. For the squared loss, we have $L'(y_i, \\hat{y}_i)=(x_i \\theta^T-y_i)x_i$.\n",
    "\n",
    "Below is a function that implements this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GD(Z,T,eta,random_seed=0):\n",
    "    (N,n)=Z.shape\n",
    "    n=n-1\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    theta_hat=np.random.rand(n)\n",
    "\n",
    "    for t in range(T):\n",
    "        sum_gradient=0\n",
    "        for i in range(N):\n",
    "            sum_gradient=sum_gradient+getGradient(Z[i,:],eta,theta_hat)\n",
    "        \n",
    "        theta_hat=theta_hat+sum_gradient/N\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Implement a function `getGradient`, that takes as inputs `z, eta and theta_hat`, and returns the gradient at z using the derivative $L'(y_i, \\hat{y}_i)=(x_i \\theta^T-y_i)x_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGradient(z,eta,theta_hat):\n",
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with `T=50`, and `eta=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_hat=GD(Z,50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Map/Reduce approach\n",
    "\n",
    "Notice that `getGradient` can be applied row-wise to the Z array, and that all gradients can be summed using a reduce action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Write a function `GD_MR` equivalent to `GD`, that implements the gradient descent in a map/reduce way. \n",
    "Tip: Start from `Z_RDD`, map `getGradient`, reduce by summing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GD_MR(Z,T,eta,random_seed=0):\n",
    "    (N,n)=Z.shape\n",
    "    n=n-1\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    theta_hat=np.random.rand(n)\n",
    "\n",
    "    for t in range(T):\n",
    "## Your solution\n",
    "## ...    \n",
    "    return theta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_hat=GD_MR(Z,50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in the Spark UI the resulting sequences of Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Linear regression with Spark ML\n",
    "\n",
    "The documentation for linear regression in Spark ML can be found at https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression. \n",
    "\n",
    "It implements the OLS and gradient descent algorithms, which can be chosen by means of the `solver` parameter:\n",
    "* `solver='normal'` : Ordinary least squares\n",
    "* `solver='l-bfgs'` : Gradient descent approach, using [the limited memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) optimisation algorihm (a more efficient version of the vanilla gradient descent we implement in section 3)\n",
    "* `solver='auto'` : Default parameter. It lets Spark choose beteen normal and l-bgfs, depending on the number of input variables (normal if chosen if number of features<=4096). \n",
    "\n",
    "In order to use the LinearRegression interface, let us first format input data in a dataframe with `label` and `features` columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "Z_RDD_tmp = map(lambda z: (float(z[-1]), Vectors.dense(z[0:-1])), Z)\n",
    "Z_DF = spark.createDataFrame(Z_RDD_tmp,schema=[\"label\", \"features\"])\n",
    "\n",
    "Z_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Fit the model with the normal and l-bgfs algorithms. \n",
    "* Check the Spark UI and compare the different stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "## Your solution\n",
    "## ...\n",
    "\n",
    "lrModel = #Your solution\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "## Relevant links\n",
    "\n",
    "* For a more practical application, see https://github.com/spark-mooc/mooc-setup/blob/master/ML_lab3_linear_reg_student.ipynb where regression with Spark ML is used to predict the year of a movie.\n",
    "\n",
    "Research articles on distributed gradient descent:\n",
    "\n",
    "* Zinkevich, M., Weimer, M., Li, L., & Smola, A. J. (2010). Parallelized stochastic gradient descent. In Advances in neural information processing systems (pp. 2595-2603). http://martin.zinkevich.org/publications/nips2010.pdf\n",
    "* Chu, C. T., Kim, S. K., Lin, Y. A., Yu, Y., Bradski, G., Olukotun, K., & Ng, A. Y. (2007). Map-reduce for machine learning on multicore. In Advances in neural information processing systems (pp. 281-288). https://papers.nips.cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
