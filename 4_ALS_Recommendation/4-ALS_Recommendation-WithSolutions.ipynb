{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [INFO-H515 - Big Data Scalable Analytics](http://uv.ulb.ac.be/course/view.php?id=74317?username=guest)\n",
    "\n",
    "\n",
    "## TP 4 - Recommender system with low rank matrix approximation and Alternating Least Squares\n",
    "\n",
    "#### *Yann-AÃ«l Le Borgne, Jacopo De Stefani and Gianluca Bontempi*\n",
    "\n",
    "####  09/05/2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class aims at implementing the Alternating Least Square (ALS) algorithm, a popular machine learning technique for recommendation systems. \n",
    "\n",
    "We will use the MovieLens dataset (https://grouplens.org/datasets/movielens/) for making recommendation on movies. The data is available from this notebook folder.\n",
    "\n",
    "### Class objectives:\n",
    "\n",
    "* Implementation of ALS in numpy\n",
    "* Implementation of ALS using Map/Reduce\n",
    "* Implementation of ALS using Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "* Recommender systems have become increasingly popular in recent years, and aim at inferring customer's preference based on their past ratings, as well as the past ratings of other customers.  \n",
    "* They are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. \n",
    "* The ratings provided by a customer usually involve a very small number of products.\n",
    "* The problem can be formalized as a matrix completion problem, where the goal is to predict the (many) missing values of a rating matrix. \n",
    "* An efficient approach to solve this problem is to rely on *low rank matrix approximation*, and the *Alternating Least Square (ALS)* algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "* Let $n_u$ and $n_p$ be the number of users and products.\n",
    "* Let $R \\in \\mathbb{R}^{n_u \\times n_p}$ be the matrix of ratings, where entry $r_{ij}$, $1 \\le i \\le n_u$ and $1 \\le j \\le n_p$, is the rating of user $i$ for product $j$. Entries $r_{ij}$ contain many missing values.\n",
    "* Let $w_{i,j}$ be an indicator of the existence of a rating for product $j$ by user $i$, i.e., $w_{i,j}=1$ if the rating $(i,j)$ exists, and $w_{i,j}=0$ otherwise.\n",
    "* Let $k$ be the rank of the matrix factorisation.\n",
    "* Let $U \\in \\mathbb{R}^{n_u \\times n_k}$ be the user matrix, and $P \\in \\mathbb{R}^{n_p \\times n_k}$ be the products (item) matrix \n",
    "* Let $U_i$ be the $i-$th row of $U$, and $P_j$ be the $j-$th row of $P$.\n",
    "\n",
    "![alt text](mat_prod.jpg \"Ratings matrix factorisation\")\n",
    "\n",
    "* Let $\\hat{R}$ be the predicted rating matrix, where all missing values are predicted on the basis of known user ratings. \n",
    "* The optimisation function is expressed as \n",
    "\n",
    "$$\n",
    "J(U,P)=||(R-UP^T)||_2\n",
    "$$\n",
    "\n",
    "and predictions $\\hat{R}$ given by \n",
    "\n",
    "$$\n",
    "\\hat{R}=UP^T\n",
    "$$\n",
    "\n",
    "### Alternating Least Squares\n",
    "\n",
    "* Note that since both $U$ and $P$ are unknown, the optimisation function $J(U,V)$ is non convex.\n",
    "* However, if we fix $P$ and optimise for $U$ alone, the problem is simply reduced to the problem of linear regression, and can be solved using Ordinary Least Square (OLS):\n",
    "\n",
    "$$\n",
    "U^T=(P^TP)^{-1}P^TR^T\n",
    "$$\n",
    "\n",
    "Then, fixing $U$ and optimising for P gives\n",
    "\n",
    "$$\n",
    "P^T=(U^TU)^{-1}U^TR\n",
    "$$\n",
    "\n",
    "\n",
    "* ALS does just that, iteratively optimising $U$ by fixing $P$, and optimising $P$ by fixing $U$. It is guaranteed to converge only to a local minima, which ultimately depends on initial values for $U$ or $P$. \n",
    "\n",
    "* **Missing values**: Since R contains missing values, regression must be computed per user (or product), using only those entries for which the ratings are known ($w_{i,j}=1$). This is done by going though all users (or products):\n",
    "    * For each i, compute $$U^T_i=(\\sum_{j, w_{i,j}=1} P_j^TP_j)^{-1} \\sum_{j, w_{i,j}=1} P_j^Tr_{ij}$$\n",
    "    * For each j, compute $$P^T_j=(\\sum_{i, w_{i,j}=1} U_i^TU_i)^{-1} \\sum_{i, w_{i,j}=1} U_i^Tr_{ij}$$\n",
    "\n",
    "\n",
    "**ALS algorithm:**\n",
    "\n",
    "1. Initialize the matrix P by assigning to the first column the average rating for each product, and using small random numbers for the remaining columns.\n",
    "2. Fix P and solve for U_i that minimizes the objective function (the root mean square error (RMSE)).\n",
    "3. Fix U and solve for P_j that minimizes the objective function similarly.\n",
    "4. Repeat steps 2 and 3 until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Let us first load the data, which is in CSV format in `ml-latest-small/ratings.csv`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv( \"ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the dataset provides the rating given by a user to a movies, as well as a timestamp. There are 100004 ratings in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       31     2.5  1260759144\n",
       "1       1     1029     3.0  1260759179\n",
       "2       1     1061     3.0  1260759182\n",
       "3       1     1129     2.0  1260759185\n",
       "4       1     1172     4.0  1260759205"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100004, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Data preprocessing consists in :\n",
    "* Dropping the timestamp column, which will not be used\n",
    "* Creating a training and validation set (random 80/20% split of the original data)\n",
    "* Reindexing the userId and movieId so their range is in the interval [0,n\\_u-1] and [0,n_p-1]\n",
    "* Finally, removing from the validation set user IDs and movie IDs that do not appear in the training set (we will not deal with the problem of rating new users/movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us drop the timestamp since we will not use it.\n",
    "rating_df=rating_df.drop( \"timestamp\", axis = 1 )\n",
    "\n",
    "#Create training and validation set\n",
    "np.random.seed(0)\n",
    "i_training = np.random.rand(len(rating_df)) < 0.8\n",
    "training_df= rating_df[i_training].reset_index(drop=True)\n",
    "validation_df= rating_df[~i_training].reset_index(drop=True)\n",
    "\n",
    "#Keep only those movies and user IDs which are in the training data (no predictions for 'new' users and movies)\n",
    "validation_df=validation_df[validation_df.movieId.isin(training_df.movieId)]\n",
    "validation_df=validation_df[validation_df.userId.isin(training_df.userId)].reset_index(drop=True)\n",
    "\n",
    "#Reindex users and movies so IDs are between 0 and numbers of users/movies\n",
    "userId=training_df.userId.unique()\n",
    "user_mapping=dict(zip(userId,range(len(userId))))\n",
    "training_df.userId=training_df.userId.map(user_mapping)\n",
    "validation_df.userId=validation_df.userId.map(user_mapping)\n",
    "\n",
    "movieId=training_df.movieId.unique()\n",
    "movie_mapping=dict(zip(movieId,range(len(movieId))))\n",
    "training_df.movieId=training_df.movieId.map(movie_mapping)\n",
    "validation_df.movieId=validation_df.movieId.map(movie_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, we have\n",
    "* 79962 ratings in the training set\n",
    "* 19281 ratings in the validation set\n",
    "* 671 unique user IDs, and 8369 unique movie IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79962, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19281, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique users\n",
    "n_u=len( training_df.userId.unique())\n",
    "n_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8369"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique movies\n",
    "n_p=len( training_df.movieId.unique())\n",
    "n_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       0        0     2.5\n",
       "1       0        1     3.0\n",
       "2       0        2     3.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>676</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>680</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2256</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       0      676     2.0\n",
       "1       0      680     3.5\n",
       "2       0     2256     4.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Baseline model - Centralised approach\n",
    "\n",
    "## 1.1) Constant model\n",
    "\n",
    "Let us first make a centralised constant model, using Pandas and numpy, where the predicted score for any movie is set to 2.5\n",
    "\n",
    "To quantify the error of the prediction, let us use the Root Mean Squard Error (RMSE), which is the root of the average quared difference between the prediction and the true value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the RMSE (root mean square error) between true ratings and predictions\n",
    "def computeRMSE(ratings,predictions):\n",
    "    \n",
    "    RMSE=np.sqrt(np.sum((predictions-ratings)**2)/len(predictions))\n",
    "    \n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute the prediction error on the training set using the constant model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings=np.array(training_df.rating)\n",
    "N=ratings.shape[0]\n",
    "predictions=np.full((N),2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4863354657446775"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRMSE(ratings,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute the prediction error on the validation set using the constant model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings=np.array(validation_df.rating)\n",
    "N=ratings.shape[0]\n",
    "predictions=np.full((N),2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.489508283150651"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRMSE(ratings,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Average model\n",
    "\n",
    "Let us now make an 'average' model, where the rating prediction for a movie is the average rating for that movie in the training set.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. Compute the average rating for each movie in the training set (use the groupby and mean function from Pandas)\n",
    "2. Compute the prediction error on the training set using the average model\n",
    "3. Compute the prediction error on the validation set using the average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_rating_movie=training_df.groupby('movieId').rating.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_predictions=average_rating_movie[training_df.movieId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88893244960704643"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRMSE(np.array(training_df.rating),np.array(training_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_predictions=average_rating_movie[validation_df.movieId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99531137099735412"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRMSE(np.array(validation_df.rating),np.array(validation_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the prediction error is higher on the validation set (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) ALS - Centralised approach\n",
    "\n",
    "Let us follow the alogrithm provided in the introduction section to compute the ALS predictions:\n",
    "\n",
    "### Initialisation\n",
    "\n",
    "* Initialise k (the ALS rank), and lambda_reg (the regularisation parameter)\n",
    "* Initialize the matrix P by assigning to the first column the average rating for each movie, and using small random numbers for the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "\n",
    "np.random.seed(0)\n",
    "P=np.random.rand(n_p,k)\n",
    "\n",
    "average_rating_movie=np.array(training_df.groupby('movieId').rating.mean())\n",
    "P[:,0]=average_rating_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8369"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS function\n",
    "\n",
    "The function aims at computing the OLS for a subset of entries of the rating matrix. It takes as inputs\n",
    "* The set of indices to keep from the rating matrix, as well as the corresponding ratings. This is provided as an array of two columns.\n",
    "* The U or P matrix, depending on which matrix is updated. This is provided as an array, called X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OLS(list_id_rating, X,lambda_reg=0.1):\n",
    "    list_id_rating=np.reshape(np.array(list_id_rating),(-1,2))\n",
    "    X=np.array(X)\n",
    "    \n",
    "    #Get the subset of rows from X for which to compute OLS\n",
    "    #Need to first convert indices to integers \n",
    "    list_id=[int(elt) for elt in list_id_rating[:,0]]\n",
    "    X_j=X[list_id,:] \n",
    "    \n",
    "    #Compute OLS\n",
    "    k=X_j.shape[1]\n",
    "    XtX=np.dot(np.transpose(X_j),X_j)+lambda_reg*np.identity(k)\n",
    "    XtY=np.dot(np.transpose(X_j),list_id_rating[:,1])\n",
    "    coefficient_OLS=np.transpose(np.dot(np.linalg.inv(XtX),XtY))\n",
    "    \n",
    "    return coefficient_OLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n"
     ]
    }
   ],
   "source": [
    "#Number of iterations\n",
    "T=1\n",
    "\n",
    "for t in range(T):\n",
    "    \n",
    "    print('Iteration: ',t)\n",
    "    \n",
    "    U=np.vstack(training_df.groupby('userId').\n",
    "                            apply(lambda list_id_rating: OLS(list_id_rating[['movieId','rating']],P)))\n",
    "    \n",
    "    P=np.vstack(training_df.groupby('movieId').\n",
    "                            apply(lambda list_id_rating: OLS(list_id_rating[['userId','rating']],U)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check results of groupby/apply transform\n",
    "#np.vstack(training_df.groupby('userId').\\\n",
    "#                            apply(lambda list_id_rating: OLS(list_id_rating[['movieId','rating']],P)))[0:4,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rating predictions and assess model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This takes U and P matrices, and compute the predictions for pairs userId/movieId in rating_df\n",
    "def getPredictions(U,P,rating_df):\n",
    "    N=rating_df.shape[0]\n",
    "    predictions=np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        predictions[i]=np.sum(np.dot(U[rating_df.userId[i],:],P[rating_df.movieId[i],:]))\n",
    "        \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80989152206399284"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_training=getPredictions(U,P,training_df)\n",
    "computeRMSE(np.array(training_df.rating),np.array(pred_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90748126399211981"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_validation=getPredictions(U,P,validation_df)\n",
    "computeRMSE(np.array(validation_df.rating),np.array(pred_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Modify the main loop of ALS to print the RMSE on the trainnig and validation set at each iteration. Modify k, lambda_reg, and T, and see how that influences the predictions. What seems to be the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Error training set: 0.809891522064\n",
      "Error validation set: 0.907481263992\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "\n",
    "np.random.seed(0)\n",
    "P=np.random.rand(n_p,k)\n",
    "average_rating_movie=np.array(training_df.groupby('movieId').rating.mean())\n",
    "P[:,0]=average_rating_movie\n",
    "\n",
    "#Number of iterations\n",
    "T=1\n",
    "\n",
    "for t in range(T):\n",
    "    \n",
    "    print('Iteration: ',t)\n",
    "    \n",
    "    U=np.vstack(training_df.groupby('userId').\n",
    "                            apply(lambda list_id_rating: OLS(list_id_rating[['movieId','rating']],P)))\n",
    "    \n",
    "    P=np.vstack(training_df.groupby('movieId').\n",
    "                            apply(lambda list_id_rating: OLS(list_id_rating[['userId','rating']],U)))\n",
    "    \n",
    "    training_predictions=getPredictions(U,P,training_df)\n",
    "    training_error=computeRMSE(np.array(training_df.rating),np.array(training_predictions))\n",
    "    \n",
    "    print(\"Error training set: \"+str(training_error))\n",
    "    \n",
    "    validation_predictions=getPredictions(U,P,validation_df)\n",
    "    validation_error=computeRMSE(np.array(validation_df.rating),np.array(validation_predictions))\n",
    "    \n",
    "    print(\"Error validation set: \"+str(validation_error))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) ALS - Map/Reduce approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Start Spark session with local master and 2 cores\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"ALS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform training and validation in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of partitions\n",
    "p=4\n",
    "\n",
    "training_df_rdd=sc.parallelize(np.array(training_df),p)\n",
    "validation_df_rdd=sc.parallelize(np.array(validation_df),p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same initialisation as centralised ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "\n",
    "np.random.seed(0)\n",
    "P=np.random.rand(n_p,k)\n",
    "\n",
    "average_rating_movie=np.array(training_df.groupby('movieId').rating.mean())\n",
    "P[:,0]=average_rating_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Adapt the main loop of the centralised ALS in a Spark Map/Reduce way.\n",
    "\n",
    "The centraliseed implementation is:\n",
    "\n",
    "```\n",
    "for t in range(T):\n",
    "    \n",
    "    print('Iteration: ',t)\n",
    "    \n",
    "    U=np.vstack(training_df.groupby('userId').\n",
    "                            apply(lambda list_id_rating: OLS(list_id_rating[['movieId','rating']],P)))\n",
    "    \n",
    "    P=np.vstack(training_df.groupby('movieId').\n",
    "                            apply(lambda list_id_rating: OLS(list_id_rating[['userId','rating']],U)))\n",
    "\n",
    "```\n",
    "\n",
    "Steps:\n",
    "\n",
    "* `training_df.groupby('userId')`: This part should be transformed so that `training_df_rdd` is first map in (key,value) pairs where th key is the userId, and the value the tuple (movieId,rating)\n",
    "* From the RDD obtained in the previous step, group by key\n",
    "* Apply the `OLS` function on the group by result using the map operator\n",
    "* Sort the result by key (using `sortByKey`)\n",
    "* Get the values (the result of the OLS function), using the `values`function\n",
    "* Collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Error training set: 0.809891522064\n",
      "Error validation set: 0.907481263992\n"
     ]
    }
   ],
   "source": [
    "T=1\n",
    "\n",
    "for t in range(T):\n",
    "    \n",
    "    print('Iteration: ',t)\n",
    "    \n",
    "    #Grouped by user ID (first column), value is (movieId,rating) (second and third columns)\n",
    "    R1=training_df_rdd.map(lambda x: (x[0],[x[1],x[2]])).groupByKey()\n",
    "    \n",
    "    U=np.array(R1.mapValues(lambda list_id_rating:OLS(list(list_id_rating),P)).\n",
    "                    sortByKey().values().collect())\n",
    "    \n",
    "    #Grouped by movie ID (second column), value is (movieId,rating) (first and third columns)\n",
    "    R2=training_df_rdd.map(lambda x: (x[1],[x[0],x[2]])).groupByKey()\n",
    "    P=np.array(R2.mapValues(lambda list_id_rating:OLS(list(list_id_rating),U)).\n",
    "                    sortByKey().values().collect())\n",
    "    \n",
    "    training_predictions=getPredictions(U,P,training_df)\n",
    "    training_error=computeRMSE(np.array(training_df.rating),np.array(training_predictions))\n",
    "    \n",
    "    print(\"Error training set: \"+str(training_error))\n",
    "    \n",
    "    validation_predictions=getPredictions(U,P,validation_df)\n",
    "    validation_error=computeRMSE(np.array(validation_df.rating),np.array(validation_predictions))\n",
    "    \n",
    "    print(\"Error validation set: \"+str(validation_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the RMSE on the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80989152206399284"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_training=getPredictions(U,P,training_df)\n",
    "computeRMSE(np.array(training_df.rating),np.array(pred_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90748126399211981"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_validation=getPredictions(U,P,validation_df)\n",
    "computeRMSE(np.array(validation_df.rating),np.array(pred_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.26142018,  2.58954558,  2.43693271,  2.26011812,  3.00573893,\n",
       "        2.75452198,  2.74514419,  2.55343655,  2.1336082 ,  2.10186937])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_training[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.5\n",
       "1    3.0\n",
       "2    3.0\n",
       "3    2.0\n",
       "4    4.0\n",
       "5    2.0\n",
       "6    2.0\n",
       "7    2.0\n",
       "8    2.5\n",
       "9    1.0\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.rating[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "Note that R1 and R2 do not need to be computed at each iteration. They can be computed before, and cached for re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Error training set: 0.809891522064\n",
      "Error validation set: 0.907481263992\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "\n",
    "np.random.seed(0)\n",
    "P=np.random.rand(n_p,k)\n",
    "\n",
    "average_rating_movie=np.array(training_df.groupby('movieId').rating.mean())\n",
    "P[:,0]=average_rating_movie\n",
    "\n",
    "\n",
    "\n",
    "R1=training_df_rdd.map(lambda x: (x[0],[x[1],x[2]])).groupByKey().cache()\n",
    "R2=training_df_rdd.map(lambda x: (x[1],[x[0],x[2]])).groupByKey().cache()\n",
    "\n",
    "T=1\n",
    "\n",
    "for t in range(T):\n",
    "    \n",
    "    print('Iteration: ',t)\n",
    "    \n",
    "    #Grouped by user ID (first column), value is (movieId,rating) (second and third columns)\n",
    "    R1=training_df_rdd.map(lambda x: (x[0],[x[1],x[2]])).groupByKey()\n",
    "    \n",
    "    U=np.array(R1.mapValues(lambda list_id_rating:OLS(list(list_id_rating),P)).\n",
    "                    sortByKey().values().collect())\n",
    "    \n",
    "    #Grouped by movie ID (second column), value is (movieId,rating) (first and third columns)\n",
    "    R2=training_df_rdd.map(lambda x: (x[1],[x[0],x[2]])).groupByKey()\n",
    "    P=np.array(R2.mapValues(lambda list_id_rating:OLS(list(list_id_rating),U)).\n",
    "                    sortByKey().values().collect())\n",
    "    \n",
    "    training_predictions=getPredictions(U,P,training_df)\n",
    "    training_error=computeRMSE(np.array(training_df.rating),np.array(training_predictions))\n",
    "    \n",
    "    print(\"Error training set: \"+str(training_error))\n",
    "    \n",
    "    validation_predictions=getPredictions(U,P,validation_df)\n",
    "    validation_error=computeRMSE(np.array(validation_df.rating),np.array(validation_predictions))\n",
    "    \n",
    "    print(\"Error validation set: \"+str(validation_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) ALS with Spark ML library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See documentation at https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the same parameters as our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10\n",
    "REG_PARAM = 0.1\n",
    "SEED_VALUE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ALS object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_479ba984b099902f858f"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als = ALS()\n",
    "als.setMaxIter(MAX_ITERATIONS)          \\\n",
    " .setSeed(SEED_VALUE)                 \\\n",
    " .setRegParam(REG_PARAM)              \\\n",
    " .setUserCol('userId')                \\\n",
    " .setItemCol('movieId')               \\\n",
    " .setRatingCol('rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RMSE evaluator using the label and predicted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training and validation sets in Spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df_rdd=spark.createDataFrame(training_df, ['userId', 'movieId', 'rating']) \n",
    "validation_df_rdd=spark.createDataFrame(validation_df, ['userId', 'movieId', 'rating']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an ALS model with ranks of increasing sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 1 the training RMSE is 0.8116843908249538\n",
      "For rank 1 the validation RMSE is 0.9118534800447594\n",
      "For rank 2 the training RMSE is 0.7506271366944117\n",
      "For rank 2 the validation RMSE is 0.9071371402911167\n",
      "For rank 5 the training RMSE is 0.6594351988007074\n",
      "For rank 5 the validation RMSE is 0.9170910150192021\n",
      "For rank 10 the training RMSE is 0.5800020605088405\n",
      "For rank 10 the validation RMSE is 0.9193936976666198\n",
      "For rank 20 the training RMSE is 0.5111833971151686\n",
      "For rank 20 the validation RMSE is 0.9143466118176342\n",
      "The best model was trained with rank 2\n"
     ]
    }
   ],
   "source": [
    "ranks = [1,2,5,10,20]\n",
    "errors_training = []\n",
    "errors_validation = []\n",
    "models = []\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "\n",
    "for i in range(len(ranks)):\n",
    "\n",
    "  # Build the model    \n",
    "  als.setRank(ranks[i])\n",
    "  model = als.fit(training_df_rdd)\n",
    "\n",
    "  # Make predictions on training dataset  \n",
    "  training_predictions_df=model.transform(training_df_rdd)\n",
    "  # Run the previously created RMSE evaluator, reg_eval, on the training_predictions_df DataFrame\n",
    "  errors_training.append(reg_eval.evaluate(training_predictions_df))\n",
    "\n",
    "  # Make predictions on validation dataset  \n",
    "  validation_predictions_df=model.transform(validation_df_rdd)\n",
    "  # Run the previously created RMSE evaluator, reg_eval, on the validation_predictions_df DataFrame\n",
    "  errors_validation.append(reg_eval.evaluate(validation_predictions_df))\n",
    "\n",
    "  print( 'For rank %s the training RMSE is %s' % (ranks[i], errors_training[i]) )\n",
    "  print( 'For rank %s the validation RMSE is %s' % (ranks[i], errors_validation[i]) )\n",
    "\n",
    "  if errors_validation[i] < min_error:\n",
    "      min_error = errors_validation[i]\n",
    "      best_rank = ranks[i]\n",
    "  \n",
    "print( 'The best model was trained with rank %s' % best_rank )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing error using computeRMSE\n",
    "validation_predictions=validation_predictions_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133</td>\n",
       "      <td>148</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.691487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>362</td>\n",
       "      <td>148</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.370282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285</td>\n",
       "      <td>148</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.029837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>325</td>\n",
       "      <td>148</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.093536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  prediction\n",
       "0     133      148     4.0    3.691487\n",
       "1     362      148     4.0    3.370282\n",
       "2     285      148     4.0    3.029837\n",
       "3     325      148     4.0    3.093536"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_predictions[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91434661181763421"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This should match the RMSE obtained for the last ALS model above\n",
    "computeRMSE(np.array(validation_predictions.rating),np.array(validation_predictions.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Koren, Yehuda, Robert Bell, and Chris Volinsky. \"Matrix factorization techniques for recommender systems.\" Computer 42.8 (2009). https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\n",
    "* http://www.awesomestats.in/spark-movie-recommendations/\n",
    "* https://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf\n",
    "* https://en.wikipedia.org/wiki/Recommender_system\n",
    "* https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/\n",
    "* https://stanford.edu/~rezab/classes/cme323/S16/projects_reports/parthasarathy_tea.pdf\n",
    "* https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala\n",
    "* http://yifanhu.net/PUB/cf.pdf\n",
    "* https://github.com/bdanalytics/Berkeley-Spark/blob/master/CS110x/cs110_lab2_als_prediction.ipynb\n",
    "* https://www.youtube.com/watch?v=RcOUXmCAssg&list=PL0Smm0jPm9WcCsYvbhPCdizqNKps69W4Z&index=95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
