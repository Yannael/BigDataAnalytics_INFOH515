{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9d0a7b",
   "metadata": {},
   "source": [
    "# [INFO-H515 - Big Data Scalable Analytics](https://uv.ulb.ac.be/course/view.php?id=85246?username=guest)\n",
    "\n",
    "## TP 7 - Spark, HDFS and Jupyter notebooks on Google Cloud\n",
    "\n",
    "*Materials originally developed by Yann-AÃ«l Le Borgne and Gianluca Bontempi*\n",
    "\n",
    "#### *Theo Verhelst, Daniele Lunghi and Gianluca Bontempi*\n",
    "\n",
    "####  Date: TBD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d15a7e",
   "metadata": {},
   "source": [
    "This class aims at illustrating how a cloud service provider such as Google can be used to launch a Spark/HDFS cluster and interact with it with a Jupyter notebook.\n",
    "\n",
    "### Class objectives:\n",
    "\n",
    "* Set up a Google Cloud account\n",
    "* Launch a Spark/HDFS cluster with DataProc\n",
    "* Start Spark jobs from a Jupyter notebook\n",
    "* Monitor jobs and cluster usage through with Spark, Yarn and HDFS web monitoring interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d35e6",
   "metadata": {},
   "source": [
    "# 1) Introduction\n",
    "\n",
    "Cloud service providers such as Google Cloud, Amazon Web Services, or Microsoft Azure all provide different solutions for setting up clusters with Big Data services such as Spark, Hadoop, or Yarn. Due to the large heterogeneity of user needs, cloud service providers offer many ways to set up and configure clusters, ranging from fine-grained solutions with detailed control on hardware and software configurations, to high-level services allowing to spin up preconfigured clusters with just a few mouse clicks. \n",
    "\n",
    "We will rely in this class on Google Cloud, and its [DataProc](https://cloud.google.com/dataproc) service. Google Cloud was chosen since, at the time of the writing of this class, it provided the safest (no charges unless explicitly authorized) and most attractive offer in terms of free credits (300 dollars over a three-month period) for testing their platform. The DataProc service was chosen since it allows to easily spin up a Spark/HDFS cluster, and interact with it from a Jupyter notebook. Note that there exists other ways to launch Spark/HDFS clusters with Google cloud, and that similar solutions exist with other cloud services providers such as AWS or Microsoft Azure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6dd96",
   "metadata": {},
   "source": [
    "# 2) Account creation \n",
    "\n",
    "\n",
    "All cloud service providers require you to create an account, verify it with your phone, and provide credit card details even if you only rely on the free credits. \n",
    "\n",
    "In order to access to Google cloud services, go to https://console.cloud.google.com. You will need to create a Google account if you don't have one.\n",
    "\n",
    "Once signed-in, you will have to accept the terms of services.\n",
    "\n",
    "<img src=\"images/Setting_Up_Account_1.jpg\"  width=\"900\"/> \n",
    "\n",
    "Note the banner at the top, which invites you to activate your free credits. \n",
    "\n",
    "<img src=\"images/Setting_Up_Account_2.jpg\"  width=\"900\"/> \n",
    "\n",
    "Click 'Activate' in order to obtain the 300 dollars of free credits. Three steps will need to be taken to obtain the credits:\n",
    "\n",
    "* First step: Select 'Belgium' as the country, and 'Other' as the organization. Agree to the terms of services.\n",
    "\n",
    "<img src=\"images/Setting_Up_Account_3.jpg\"  width=\"900\"/> \n",
    "\n",
    "* Second step: Enter your phone number and click on 'Send code' to obtain a validation code on your phone. Verify your account with the 6-digit code sent to your phone.\n",
    "\n",
    "* Third step: Verify your payment information. For account 'type', select 'individual'. Enter your address, and choose your payment method. Finally, click on 'Start my free trial'. \n",
    "\n",
    "You may get a pop-up window asking your for further verification (e.g., 3D-secure verification). Follow the verification procedure by clicking on 'Continue'.\n",
    "\n",
    "<img src=\"images/Setting_Up_Account_4.jpg\"  width=\"500\"/> \n",
    "\n",
    "\n",
    "**Important notes regarding billing:** \n",
    "\n",
    "* **Important note 1:** In our experience, Google will not charge you unless you explicitly authorize it to do so. That is, once you run out of free credits, access to paid services will simply be suspended. **Keep in mind that this may change in the future, and do not take for granted that you will not be billed**.    \n",
    "\n",
    "* **Important note 2:** Billing concepts for cloud services are very complex. See here for an overview: https://cloud.google.com/billing/docs/concepts?hl=en-GB. \n",
    "\n",
    "* **Important note 3:** You may (and will) have bad surprises regarding billing. It is common, when starting a service that other services are also activated without you noticing. For example, starting a Spark cluster implicitly creates a Google bucket, for which you will also be charged. If you delete your cluster, but forget to delete the bucket, you will be charged until you delete the bucket. Another example concerns queries on big datasets, using for example the Google BigQuery service. In that case, billing depends on the amount of data that needed to be processed in order to answer your query. Even if your query seems simple (like select something from a table), if the table is huge, charges may be significant. There are many other examples of unexpected charges. **Therefore, always read carefully the documentation for the services you try**. \n",
    "\n",
    "* **Important note 4:** **Always delete everything you created after using a service, unless you want to keep it (and therefore pay for it)**. The last step of this class consists in properly deleting all of the services that were used. **Follow it carefully**.\n",
    "\n",
    "Useful links regarding billing:\n",
    "\n",
    "* Google Cloud free program: https://cloud.google.com/free/docs/gcp-free-tier\n",
    "* Overview of Cloud Billing concepts: https://cloud.google.com/billing/docs/concepts?hl=en-GB\n",
    "* Understanding Google Cloud costs: https://www.cloudskillsboost.google/quests/90?qlcampaign=1q-cloudent-061&utm_source=qwiklabs&utm_medium=console&utm_campaign=pilot&hl=en-GB\n",
    "* Quota requests: https://support.google.com/cloud/answer/6330231\n",
    "\n",
    "Your billing details can be found in the navigation menu, under 'Billing'. Go to 'Overview'. On the dashboard, your free remaining credits should appear on the left side of the screen, at the bottom. \n",
    "\n",
    "<img src=\"images/Setting_Up_Account_5.jpg\"  width=\"350\"/> \n",
    "\n",
    "\n",
    "**Check the dashboard regularly, even when not using services, in order to avoid being charged for a service you would have forgotten to properly shut down**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c936076",
   "metadata": {},
   "source": [
    "# 3) Set up Google Dataproc\n",
    "\n",
    "[Google Dataproc](https://cloud.google.com/dataproc) is the Google service for launching Spark/HDFS clusters and running Spark jobs. In the following, we show how to create and configure a new project on Dataproc for launching a Spark/HDFS cluster.\n",
    "\n",
    "The total cost to run this lab on Google Cloud is about 2 dollars. See the [Dataproc pricing details here](https://cloud.google.com/dataproc/pricing?authuser=2). The last section of this class will detail how to clean up your project. \n",
    "\n",
    "### Create a new project\n",
    "\n",
    "Sign-in to Google Cloud Platform console at [console.cloud.google.com](console.cloud.google.com) and create a new project:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>1)</td>\n",
    "        <td>\n",
    "            <img src=\"images/Create_Project_1.png\"  width=\"550\"/> \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2)</td>\n",
    "        <td>\n",
    "            <img src=\"images/Create_Project_2a.jpg\"  width=\"700\"/> \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3)</td>\n",
    "        <td>\n",
    "            <img src=\"images/Create_Project_3a.jpg\"  width=\"600\"/> \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3d08b",
   "metadata": {},
   "source": [
    "### Set up your environment\n",
    "\n",
    "First, open up Cloud Shell by clicking the button in the top right-hand corner of the cloud console:\n",
    "\n",
    "<img src=\"images/Set_up_environment_1.png\"  width=\"600\"/> \n",
    "\n",
    "After the Cloud Shell loads, run the following command to set the project ID from the previous step:\n",
    "\n",
    "```\n",
    "gcloud config set project <project_id>\n",
    "\n",
    "```\n",
    "\n",
    "The project ID can also be found by clicking on your project in the top left of the cloud console:\n",
    "\n",
    "<img src=\"images/Set_up_environment_2.png\"  width=\"500\"/> \n",
    "\n",
    "<img src=\"images/Set_up_environment_3.png\"  width=\"600\"/> \n",
    "\n",
    "You be may be asked to authorize Google to make a GCP API call. If so, authorize.\n",
    "\n",
    "Next, enable the Dataproc, Compute Engine and Storage Components APIs.\n",
    "\n",
    "```\n",
    "gcloud services enable dataproc.googleapis.com \\\n",
    "  compute.googleapis.com \\\n",
    "  storage-component.googleapis.com \n",
    "```\n",
    "\n",
    "Enabling APIs can take up to one minute.\n",
    "\n",
    "Alternatively this can be done in the Cloud Console. Click on the menu icon in the top left of the screen.\n",
    "\n",
    "<img src=\"images/Set_up_environment_4.png\"  width=\"300\"/> \n",
    "\n",
    "\n",
    "Select API Manager from the drop down.\n",
    "\n",
    "\n",
    "<img src=\"images/Set_up_environment_5.png\"  width=\"300\"/> \n",
    "\n",
    "Click on Enable APIs and Services.\n",
    "\n",
    "<img src=\"images/Set_up_environment_6.png\"  width=\"500\"/> \n",
    "\n",
    "Search for and enable the following APIs:\n",
    "\n",
    "* Compute Engine API\n",
    "* Dataproc API\n",
    "* Storage Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a20db",
   "metadata": {},
   "source": [
    "# 4) Create a Spark/HDFS cluster\n",
    "\n",
    "## Creating your cluster\n",
    "\n",
    "Set the environment variables for your cluster (in the Cloud shell):\n",
    "\n",
    "```\n",
    "REGION=europe-west1\n",
    "CLUSTER_NAME=<project_id>\n",
    "```\n",
    "\n",
    "Then run this gcloud command to create your cluster with all the necessary components to work with Jupyter on your cluster. We will use two `n1-standard-8` machines, which feature [4 vCPUs and 30GB RAM](https://cloud.google.com/compute/all-pricing) and cost around 0.4 dollar/hour.\n",
    "\n",
    "```\n",
    "gcloud beta dataproc clusters create ${CLUSTER_NAME} \\\n",
    " --region=${REGION} \\\n",
    " --image-version=1.5 \\\n",
    " --master-machine-type=n1-standard-8 \\\n",
    " --worker-machine-type=n1-standard-8 \\\n",
    " --num-workers=2 \\\n",
    " --optional-components=ANACONDA,JUPYTER \\\n",
    " --enable-component-gateway \n",
    "```\n",
    "\n",
    "You should see the following output while your cluster is being created\n",
    "\n",
    "```\n",
    "Waiting on operation [projects/cluster-hdfs-spark/regions/europe-west1/operations/abcd123456].\n",
    "Waiting for cluster creation operation...\n",
    "```\n",
    "\n",
    "It should take about 90 seconds to create your cluster and once it is ready you will be able to access your cluster from the [Dataproc Cloud console UI](https://console.cloud.google.com/dataproc/clusters?authuser=2).\n",
    "\n",
    "While you are waiting you can carry on reading below to learn more about the flags used in gcloud command (description for all the flags at https://cloud.google.com/sdk/gcloud/reference/beta/dataproc/clusters/create).\n",
    "\n",
    "You should the following output once the cluster is created:\n",
    "\n",
    "```\n",
    "Created [https://dataproc.googleapis.com/v1beta2/projects/project-id/regions/europe-west1/clusters/spark-jupyter] Cluster placed in zone [europe-west1-c].\n",
    "```\n",
    "\n",
    "#### Flags used in gcloud dataproc create command\n",
    "\n",
    "Here is a breakdown of the flags used in the gcloud dataproc create command\n",
    "\n",
    "```\n",
    "--region=${REGION}\n",
    "```\n",
    "\n",
    "Specifies the region where the cluster will be created. You can see the list of available regions [here](https://cloud.google.com/compute/docs/regions-zones).\n",
    "\n",
    "```\n",
    "--image-version=1.5\n",
    "```\n",
    "\n",
    "The image version to use in your cluster. You can see the list of available versions [here](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions).\n",
    "\n",
    "\n",
    "```\n",
    "--master-machine-type=n1-standard-8\n",
    "--worker-machine-type=n1-standard-8\n",
    "```\n",
    "\n",
    "The machine types to use for your Dataproc cluster. You can see a list of available machine types [here](https://cloud.google.com/compute/docs/machine-types), and princing detail [here](https://cloud.google.com/compute/all-pricing).\n",
    "\n",
    "By default, 1 master node and 2 worker nodes are created if you do not set the flag â-num-workers\n",
    "\n",
    "```\n",
    "--optional-components=ANACONDA,JUPYTER\n",
    "```\n",
    "\n",
    "Setting these values for optional components will install all the necessary libraries for Jupyter and Anaconda (which is required for Jupyter notebooks) on your cluster.\n",
    "\n",
    "```\n",
    "--enable-component-gateway\n",
    "```\n",
    "\n",
    "Enabling Component Gateway creates an App Engine link using Apache Knox and Inverting Proxy which gives easy, secure and authenticated access to the Jupyter and JupyterLab web interfaces meaning you no longer need to create SSH tunnels.\n",
    "\n",
    "It will also create links for other tools on the cluster including the Yarn Resource manager and Spark History Server which are useful for seeing the performance of your jobs and cluster usage patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dfa04b",
   "metadata": {},
   "source": [
    "# 5) Upload a Spark notebook\n",
    "\n",
    "#### Accessing the JupyterLab web interface\n",
    "\n",
    "Once the cluster is ready you can find the Component Gateway link to the JupyterLab web interface by going to [Dataproc Clusters - Cloud console](https://console.cloud.google.com/dataproc/clusters?authuser=2), clicking on the cluster you created and going to the Web Interfaces tab. The Dataproc cluster console can also be found under 'Big Data'/'Dataproc'/'Clusters' in the Cloud console main menu (top left of the UI).\n",
    "\n",
    "<img src=\"images/DataProc_UI.gif\"  width=\"900\"/> \n",
    "\n",
    "You will notice that you have access to Jupyter which is the classic notebook interface or JupyterLab which is described as the next-generation UI for Project Jupyter.\n",
    "\n",
    "There are a lot of great new UI features in JupyterLab and so if you are new to using notebooks or looking for the latest improvements it is recommended to go with using JupyterLab as it will eventually replace the classic Jupyter interface according to the official docs.\n",
    "\n",
    "However, JupyterLab in DataProc does not easily support plotting with Matplotlib. Therefore, for this class where we require plotting some results, select the Jupyter interface.\n",
    "\n",
    "Notice that this web interface page also gives you access to the Spark history server, the YARN ResourceManager, and the HDFS NameNode web interface. \n",
    "\n",
    "#### Upload a notebook\n",
    "\n",
    "From the Jupyter interface, **go to GCS folder**, and upload the 7-FeatureSelection-GoogleCloud notebook (if you do not go to GCS folder, the upload will hang since you do not have write permission on the root folder). \n",
    "\n",
    "The notebook is an adapted version of the notebook used for the class on feature selection. The main differences are:\n",
    "\n",
    "* A dataset with 1000 observations and 30000 features is used\n",
    "* The sections for data visualization and ranking with correlation are removed. Experiments are made with mRMR\n",
    "* The master is changed to 'yarn' for Spark, where you can vary the number of instances and cores\n",
    "* We finally show how data can be stored and then loaded from HDFS for faster execution.\n",
    "\n",
    "<img src=\"images/Jupyter_UI.gif\"  width=\"900\"/> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb43339",
   "metadata": {},
   "source": [
    "# 6) Feature selection with mRMR\n",
    "\n",
    "\n",
    "As for TP 6, import the required libraries, and generate an artificial dataset (Section 1 of the notebook). The dataset contains 1000 observations and 10000 informative features, 10000 noisy features and 10000 redundant features (30000 features in total).\n",
    "\n",
    "##  Centralized approach\n",
    "\n",
    "Run the centralized approach. Note that the time to select a feature linearly increases with the number of features, each step taking on average 2.5 seconds more than the previous step. The first step takes around 2.5 seconds, while the tenth step takes around 25 seconds.\n",
    "\n",
    "<img src=\"images/Execution_times_1.jpg\"  width=\"600\"/> \n",
    "\n",
    "## Spark and Map/Reduce\n",
    "\n",
    "### 1 instance, 1 core\n",
    "\n",
    "With 1 instance and 1 core, the overhead of Spark and Yarn makes the execution around 15 seconds longer for each step of the algorithm. The increase of execution times for each new selected feature is however the same as the centralized approach, that is, around 2.5 seconds. \n",
    "\n",
    "<img src=\"images/Execution_times_2.jpg\"  width=\"600\"/> \n",
    "\n",
    "You can check in the Spark History server how tasks were distributed during the last step of the feature selection. Since there is only one instance with one core, the 16 partitions are processed one after another.\n",
    "\n",
    "<img src=\"images/SparkHS_UI_1.gif\"  width=\"900\"/> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fb72e",
   "metadata": {},
   "source": [
    "### 2 instances, 8 cores\n",
    "\n",
    "The tasks are now distributed on the 16 cores available on the two instances. The overhead of Spark and Yarn is reduced. The increase of execution time for each new selected feature is divided by a factor of 10, down to around 0.25 seconds.\n",
    "\n",
    "<img src=\"images/Execution_times_3.jpg\"  width=\"600\"/> \n",
    "\n",
    "You can check in the Spark UI how tasks were distributed during the last step of the feature selection. Since there are 2 instances with 8 cores, the 16 partitions are processed in parallel.\n",
    "\n",
    "Note: Since the Spark session is still active, we need to go to the Spark UI instead of the Spark history server. The Spark UI can be accessed from the Yarn ResourceManager.\n",
    "\n",
    "<img src=\"images/Spark_UI_1.gif\"  width=\"900\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f44d94",
   "metadata": {},
   "source": [
    "### 2 instance, 8 cores, using HDFS\n",
    "\n",
    "Using HDFS allows to reduce the overhead due to data transfer. The increase of execution time for each new selected feature remains at around 0.25 seconds. \n",
    "\n",
    "<img src=\"images/Execution_times_4.jpg\"  width=\"600\"/> \n",
    "\n",
    "We can use the HDFS NameNode UI to see how much space was used on HDFS, and how many replicas exist for the data (2 in this setup).\n",
    "\n",
    "<img src=\"images/HDFS_UI_1.gif\"  width=\"900\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd634f56",
   "metadata": {},
   "source": [
    "# 7) Clean up your resources\n",
    "\n",
    "To avoid incurring unnecessary charges to your GCP account after completion of this lab:\n",
    "\n",
    "* [Delete the Cloud Storage bucket](https://cloud.google.com/storage/docs/deleting-buckets) for the environment and that you created\n",
    "* [Delete the Dataproc environment](https://cloud.google.com/dataproc/docs/guides/manage-cluster).\n",
    "\n",
    "If you created a project just for this lab, you can also optionally delete the project:\n",
    "\n",
    "* In the GCP Console, go to the Projects page.\n",
    "* In the project list, select the project you want to delete and click Delete.\n",
    "* In the box, type the project ID, and then click Shut down to delete the project.\n",
    "\n",
    "Caution: Deleting a project has the following effects:\n",
    "\n",
    "* Everything in the project is deleted. If you used an existing project for this tutorial, when you delete it, you also delete any other work you've done in the project.\n",
    "* Custom project IDs are lost. When you created this project, you might have created a custom project ID that you want to use in the future. To preserve the URLs that use the project ID, such as an appspot.com URL, delete selected resources inside the project instead of deleting the whole project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597e0bf",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* Part of this lab was inspired by the following tutorial: Apache Spark and Jupyter Notebooks on Cloud Dataproc - https://codelabs.developers.google.com/codelabs/spark-jupyter-dataproc#0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
